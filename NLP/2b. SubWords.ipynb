{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\arthu\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a93142155554a0d8d1aa77706bb1791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c4ee0f35864558aff07fc9779d1bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b60f95237849fb82142bcb5f5c486b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a2f3a1363848f88d0e7114cf15a19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f425e633a2c4c9b9bf863c116231084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\arthu\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteLNNBW2\\imdb_reviews-train…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9766770cfa264eaaa7b303ea7b4df751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff83d590ea7b499e9235d705501226c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\arthu\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteLNNBW2\\imdb_reviews-test.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe40571019240c183c5b8b8579b91dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1952ee1d826412699eea782437e50a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling C:\\Users\\arthu\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0.incompleteLNNBW2\\imdb_reviews-unsup…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Dataset is using deprecated text encoder API which will be removed soon. Please use the plain_text version of the dataset and migrate to `tensorflow_text`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\arthu\\tensorflow_datasets\\imdb_reviews\\subwords8k\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Download the plain text default config\n",
    "imdb_plaintext, info_plaintext = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
    "\n",
    "# Download the subword encoded pretokenized dataset\n",
    "imdb_subwords, info_subwords = tfds.load(\"imdb_reviews/subwords8k\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
       "    'text': Text(shape=(), dtype=string),\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print description of features\n",
    "info_plaintext.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"  \n",
      " Labelnya 0\n",
      "b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.'  \n",
      " Labelnya 0\n"
     ]
    }
   ],
   "source": [
    "# Take 2 training examples and print the text feature\n",
    "for example in imdb_plaintext['train'].take(2):\n",
    "  print(example[0].numpy(),\" \\n Labelnya\",example[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeaturesDict({\n",
       "    'label': ClassLabel(shape=(), dtype=int64, num_classes=2),\n",
       "    'text': Text(shape=(None,), dtype=int64, encoder=<SubwordTextEncoder vocab_size=8185>),\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print description of features\n",
    "info_subwords.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  62   18   41  604  927   65    3  644 7968   21   35 5096   36   11\n",
      "   43 2948 5240  102   50  681 7862 1244    3 3266   29  122  640    2\n",
      "   26   14  279  438   35   79  349  384   11 1991    3  492   79  122\n",
      "  188  117   33 4047 4531   14   65 7968    8 1819 3947    3   62   27\n",
      "    9   41  577 5044 2629 2552 7193 7961 3642    3   19  107 3903  225\n",
      "   85  198   72    1 1512  738 2347  102 6245    8   85  308   79 6936\n",
      " 7961   23 4981 8044    3 6429 7961 1141 1335 1848 4848   55 3601 4217\n",
      " 8050    2    5   59 3831 1484 8040 7974  174 5773   22 5240  102   18\n",
      "  247   26    4 3903 1612 3902  291   11    4   27   13   18 4092 4008\n",
      " 7961    6  119  213 2774    3   12  258 2306   13   91   29  171   52\n",
      "  229    2 1245 5790  995 7968    8   52 2948 5240 8039 7968    8   74\n",
      " 1249    3   12  117 2438 1369  192   39 7975]  \n",
      " Labelnya 0\n",
      "[  12   31   93  867    7 1256 6585 7961  421  365    2   26   14    9\n",
      "  988 1089    7    4 6728    6  276 5760 2587    2   81 6118 8029    2\n",
      "  139 1892 7961    5 5402  246   25    1 1771  350    5  369   56 5397\n",
      "  102    4 2547    3 4001   25   14 7822  209   12 3531 6585 7961   99\n",
      "    1   32   18 4762    3   19  184 3223   18 5855 1045    3 4232 3337\n",
      "   64 1347    5 1190    3 4459    8  614    7 3129    2   26   22   84\n",
      " 7020    6   71   18 4924 1160  161   50 2265    3   12 3983    2   12\n",
      "  264   31 2545  261    6    1   66    2   26  131  393    1 5846    6\n",
      "   15    5  473   56  614    7 1470    6  116  285 4755 2088 7961  273\n",
      "  119  213 3414 7961   23  332 1019    3   12 7667  505   14   32   44\n",
      "  208 7975]  \n",
      " Labelnya 0\n"
     ]
    }
   ],
   "source": [
    "# Take 2 training examples and print its contents\n",
    "for example in imdb_subwords['train'].take(2):\n",
    "  print(example[0].numpy(),\" \\n Labelnya\",example[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n"
     ]
    }
   ],
   "source": [
    "# Get the encoder\n",
    "tokenizer_subwords = info_subwords.features['text'].encoder\n",
    "\n",
    "# Take 2 training examples and decode the text feature\n",
    "for example in imdb_subwords['train'].take(2):\n",
    "  print(tokenizer_subwords.decode(example[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the train set\n",
    "train_data = imdb_plaintext['train']\n",
    "\n",
    "# Initialize sentences list\n",
    "training_sentences = []\n",
    "\n",
    "# Loop over all training examples and save to the list\n",
    "for s,_ in train_data:\n",
    "  training_sentences.append(s.numpy().decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n"
     ]
    }
   ],
   "source": [
    "print(training_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 10000\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer_plaintext = Tokenizer(num_words = 10000, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary for the training sentences\n",
    "tokenizer_plaintext.fit_on_texts(training_sentences)\n",
    "\n",
    "# Generate the training sequences\n",
    "sequences = tokenizer_plaintext.texts_to_sequences(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12,14,33,425,392,18,90,28,1,9,32,1366,3585,40,486,1,197,24,85,154,19,12,213,329,28,66,247,215,9,477,58,66,85,114,98,22,5675,12,1322,643,767,12,18,7,33,400,8170,176,2455,416,2,89,1231,137,69,146,52,2,1,7577,69,229,66,2933,16,1,2904,1,1,1479,4940,3,39,3900,117,1584,17,3585,14,162,19,4,1231,917,7917,9,4,18,13,14,4139,5,99,145,1214,11,242,683,13,48,24,100,38,12,7181,5515,38,1366,1,50,401,11,98,1197,867,141,10\n"
     ]
    }
   ],
   "source": [
    "print(*sequences[0],sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this was an absolutely terrible movie don't be <OOV> in by christopher walken or michael <OOV> both are great actors but this must simply be their worst role in history even their great acting could not redeem this movie's ridiculous storyline this movie is an early nineties us propaganda piece the most pathetic scenes were those when the <OOV> rebels were making their cases for <OOV> maria <OOV> <OOV> appeared phony and her pseudo love affair with walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning i am disappointed that there are movies like this ruining actor's like christopher <OOV> good name i could barely sit through it\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the first sequence using the Tokenizer class\n",
    "tokenizer_plaintext.sequences_to_texts(sequences[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88583"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of words in the word index dictionary\n",
    "len(tokenizer_plaintext.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SubwordTextEncoder' object has no attribute 'word_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Subword text encoding gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words. See how these subwords look like for this particular encoder:\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# Print the subwords\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mprint\u001b[39m(tokenizer_subwords\u001b[39m.\u001b[39;49mword_index)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SubwordTextEncoder' object has no attribute 'word_index'"
     ]
    }
   ],
   "source": [
    "\n",
    "#Subword text encoding gets around this problem by using parts of the word to compose whole words. This makes it more flexible when it encounters uncommon words. See how these subwords look like for this particular encoder:\n",
    "\n",
    "# Print the subwords\n",
    "print(tokenizer_subwords.subwords[61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 18, 41, 604, 927, 65, 3, 644, 7968, 21, 35, 5096, 36, 11, 43, 2948, 5240, 102, 50, 681, 7862, 1244, 3, 3266, 29, 122, 640, 2, 26, 14, 279, 438, 35, 79, 349, 384, 11, 1991, 3, 492, 79, 122, 188, 117, 33, 4047, 4531, 14, 65, 7968, 8, 1819, 3947, 3, 62, 27, 9, 41, 577, 5044, 2629, 2552, 7193, 7961, 3642, 3, 19, 107, 3903, 225, 85, 198, 72, 1, 1512, 738, 2347, 102, 6245, 8, 85, 308, 79, 6936, 7961, 23, 4981, 8044, 3, 6429, 7961, 1141, 1335, 1848, 4848, 55, 3601, 4217, 8050, 2, 5, 59, 3831, 1484, 8040, 7974, 174, 5773, 22, 5240, 102, 18, 247, 26, 4, 3903, 1612, 3902, 291, 11, 4, 27, 13, 18, 4092, 4008, 7961, 6, 119, 213, 2774, 3, 12, 258, 2306, 13, 91, 29, 171, 52, 229, 2, 1245, 5790, 995, 7968, 8, 52, 2948, 5240, 8039, 7968, 8, 74, 1249, 3, 12, 117, 2438, 1369, 192, 39, 7975]\n",
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n"
     ]
    }
   ],
   "source": [
    "# Encode the first plaintext sentence using the subword text encoder\n",
    "tokenized_string = tokenizer_subwords.encode(training_sentences[0])\n",
    "print(tokenized_string)\n",
    "\n",
    "# Decode the sequence\n",
    "original_string = tokenizer_subwords.decode(tokenized_string)\n",
    "\n",
    "# Print the result\n",
    "print (original_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [[1, 37, 1, 6, 1]]\n",
      "The original string: ['<OOV> from <OOV> to <OOV>']\n"
     ]
    }
   ],
   "source": [
    "# Define sample sentence\n",
    "sample_string = 'TensorFlow, from basics to mastery'\n",
    "\n",
    "# Encode using the plain text tokenizer\n",
    "tokenized_string = tokenizer_plaintext.texts_to_sequences([sample_string])\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the result\n",
    "original_string = tokenizer_plaintext.sequences_to_texts(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [6307, 2327, 4043, 2120, 2, 48, 4249, 4429, 7, 2652, 8050]\n",
      "The original string: TensorFlow, from basics to mastery\n"
     ]
    }
   ],
   "source": [
    "# Encode using the subword text encoder\n",
    "tokenized_string = tokenizer_subwords.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "# Decode and print the results\n",
    "original_string = tokenizer_subwords.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6307 ----> Ten\n",
      "2327 ----> sor\n",
      "4043 ----> Fl\n",
      "2120 ----> ow\n",
      "2 ----> , \n",
      "48 ----> from \n",
      "4249 ----> basi\n",
      "4429 ----> cs \n",
      "7 ----> to \n",
      "2652 ----> master\n",
      "8050 ----> y\n"
     ]
    }
   ],
   "source": [
    "# Show token to subword mapping:\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_subwords.decode([ts])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
